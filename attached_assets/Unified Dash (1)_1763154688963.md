## **`Objective Summary`**

`Evolve Unified Dash from a links hub into a live, AI‑native control plane that auto‑discovers services, surfaces health and utilization, orchestrates jobs across GPUs/containers, and unifies Home Assistant events and AI workflows in one place. This builds directly on your current topology (big‑box + mini‑sparx + HA) and the initial dash concept you drafted (discovery/routing, dashboard UI, auth/proxy, auto‑registration, monitoring).`

---

## **`Component Breakdown`**

**`Inputs`**

* **`Service inventory & health:`** `MCP on mini‑sparx (/api/system/status, /api/docker/containers), Docker APIs, Caddy, Langfuse, and service UIs (Flowise, n8n, Ollama, LocalAI, Supabase, Qdrant, Neo4j, ClickHouse, Redis).`

* **`Compute/usage metrics:`** `GPU/VRAM per big‑box (GPU0 ~76% utilized vs GPU1 ~1%), CPU/RAM on mini‑sparx (101 GB free), storage levels (e.g., Models drive 94% full).`

* **`Automation/events:`** `Home Assistant entities (443), cameras (7), motion events, and the Vision Caption API (operational; Tracker camera).`

**`Processes`**

* `Auto‑discovery (MCP/Docker), health polling, GPU/RAM/storage telemetry, event ingestion (HA), and inference observability (Langfuse).`

**`Outputs`**

* **`Dashboards:`** `Compute health, service status, camera/vision timeline, automation coverage, model registry.`

* **`Actions:`** `Open service, (re)start/redeploy container, route/schedule AI jobs to an idle GPU, run HA automations, launch n8n/Flowise flows, create data pipelines, and raise alerts.`

  ---

  ## **`Reconstruction Plan (first‑principles improvements)`**

1. **`Real‑time Service Graph (auto‑discovery)`**

* `Use MCP and Docker to enumerate running containers/services and tag them via labels (ai, db, automation, vision, infra). Cards render dynamically; no hard‑coded URLs. The plan aligns with your original “discovery & routing layer”.`

2. **`Compute & Capacity Pane (actionable)`**

* **`GPU scheduler:`** `Explicitly expose GPU0 vs GPU1 queues; place long‑running or batch inference on the idle GPU1 (~96 GB free) by default; show per‑queue wait time and VRAM headroom.`

* **`Storage policy bar:`** `Warn at 85% and block at 95% on the Models volume (currently 94%). Provide one‑click offload to MinIO on mini‑sparx and symlink back to the big‑box path.`

* **`Mini‑sparx RAM harness:`** `Encourage memory‑heavy vector/graph ops on mini‑sparx (≈101 GB free) and expose “Run on mini‑sparx” toggles for Qdrant/Neo4j jobs.`

3. **`Vision Operations Hub`**

* **`Camera coverage matrix:`** `Show which of the 7 Reolinks feed the Vision Caption API; add per‑camera toggles + rate limits. Today only Tracker is wired in; surface that and nudge to expand.`

* **`Caption stream:`** `Persist captions to ClickHouse with time filters, searchable by entity/camera; add “open snapshot” and “re‑caption on VLM X” actions.`

* **`Automation builder:`** `From any caption, generate HA automations or n8n workflows with pre‑filled triggers (motion → snapshot → caption → route).`

4. **`Automation Coverage & Opportunities`**

* **`Coverage KPI:`** `“Automations using entities” vs 443 total; you currently have 1 active automation → surface gaps and propose recipes (lights, energy, presence, security), then one‑click deploy via HA or n8n.`

* **`Recipe library:`** `Curated templates (e.g., “Unknown person at night → turn on porch lights; notify + snapshot series; store in MinIO”).`

5. **`Model Registry & Placement`**

* `Unified registry listing LMStudio, Ollama, LocalAI models with placement (GPU0/GPU1/CPU), VRAM footprint, and typical latency; “pin” critical models and “spill” non‑critical to GPU1. LMStudio and Ollama are already deployed—make selection first‑class.`

6. **`Observability & Runbooks`**

* **`Langfuse panel`** `for LLM call volume/latency/feedback; red/amber/green per service.`

* **`Self‑healing policies:`** `E.g., if ComfyUI or InvokeAI crash or sit idle for N days (large disk installs), auto‑sleep or prune cache; single‑click wake.`

* **`SLOs:`** `“Vision latency < 3 s,” “GPU queue wait < 30 s,” “Caption success > 99%.” Breaches raise unified alerts.`

7. **`Security & Access`**

* `Consolidate behind Caddy with internal SSO, per‑role access (Operator/Builder/Viewer), and signed iFrame embedding for sensitive UIs (Supabase, Neo4j). Caddy is already part of mini‑sparx.`

8. **`Task‑Oriented AI Command Bar`**

* `Natural‑language router: “caption all cameras 5 min,” “open Langfuse,” “move 200 GB of flux models to MinIO,” “deploy Qdrant 1.8.” Routes to MCP/HA/n8n/Flowise with dry‑run + confirm.`

9. **`Data Workbench Shortcuts`**

* **`Qdrant/Neo4j/ClickHouse`** `query pads with NL→Query translation (LLM‑assisted) and saved notebooks; send results to Supabase tables or MinIO buckets.`

  ---

  ## **`AI Integration Points`**

* **`LLM Router:`** `Parses free‑text commands, maps to actions (open, query, schedule, move, deploy), returns structured intents.`

* **`GPU Scheduler Policy:`** `LLM suggests placement (latency vs cost vs VRAM); human confirm then execute.`

* **`NL→Query:`** `Translate natural language into Qdrant filters, Cypher (Neo4j), and SQL (ClickHouse/Postgres).`

* **`Automation Recommender:`** `Mine HA entities and recent events to propose high‑value automations; generate YAML or n8n flows.`

* **`Incident Copilot:`** `Summarize Langfuse traces, container restarts, disk pressure; propose remediation runbook.`

  ---

  ## **`Suggested Tools or Frameworks`**

* **`Front‑end:`** `Your current React/Tailwind or a lightweight dash framework; keep cards modular and data‑driven. The earlier doc notes React or low‑code options (Dashy/Flame/Homer) if you choose to accelerate.`

* **`Back‑end:`** `FastAPI or Node/Express broker that talks to MCP, Docker, HA, Langfuse, and Caddy.`

* **`Auth/Proxy:`** `Caddy (already deployed) for TLS, auth, signed embed.`

* **`Telemetry:`** `nvidia-smi/DCGM for GPU; Docker stats; ClickHouse for time‑series; Langfuse for LLM traces.`

  ---

  ## **`LLM Implementation Instructions`**

**`Global constraints`**

* `Work offline‑first using local endpoints (MCP/HA/Docker/LMStudio/Ollama/LocalAI).`

* **`Never`** `execute a destructive action without emitting a dry‑run plan and receiving confirmation.`

* `All tool calls must return JSON with status, action, inputs, expect, rollback.`

* `Log every action to Langfuse with session_id, intent, tools_called, outcome.`

  ### **`1) Router Agent (natural‑language → intent)`**

**`System prompt (summary)`**

* `You are the Router. Convert user text into an explicit intent and tool sequence. Prefer local services. If uncertain, ask for the single most critical clarification.`

**`Output schema`**

* `{`  
  *   `"intent": "open_service | run_query | schedule_job | manage_storage | manage_automation | vision_task | unknown",`  
  *   `"confidence": 0.0,`  
  *   `"tools": [`  
  *     `{"name": "mcp", "op": "list|exec|fs", "args": {}},`  
  *     `{"name": "docker", "op": "stats|restart|logs", "args": {}},`  
  *     `{"name": "ha", "op": "get|set|automation.create|automation.toggle|camera.snapshot", "args": {}},`  
  *     `{"name": "gpu", "op": "schedule|status", "args": {}},`  
  *     `{"name": "storage", "op": "minio.move|fs.symlink", "args": {}}`  
  *   `],`  
  *   `"guardrails": ["dry_run", "require_confirm_if_write=true"]`  
  * `}`  
    

    ### **`2) GPU Scheduler Policy Agent`**

**`System prompt (summary)`**

* `Assign jobs to GPU with lowest expected wait while meeting VRAM needs. Bias to GPU1 when idle unless a model is already hot‑loaded on GPU0 and latency is critical. Current signals: GPU utilization/VRAM, job class, model footprint.`

**`Input`**

* `{"jobs":[{"name":"caption_batch","vram_gb":22,"latency":"normal"}],`  
  *  `"gpus":[{"id":0,"util":0.76,"vram_free_gb":20},{"id":1,"util":0.01,"vram_free_gb":96}]}`  
    

**`Output`**

* `{"placement":[{"job":"caption_batch","gpu":1,"reason":"GPU1 idle, ample VRAM"}]}`


  ### **`3) NL→Query Agents`**

* **`ClickHouse SQL Agent:`** `Map “show last 24h captions with motion” → SQL over captions table.`

* **`Neo4j Cypher Agent:`** `Map “list cameras within 10 m of driveway and linked automations” → Cypher.`

* **`Qdrant Filter Agent:`** `Map “find similar captions to ‘person with package’ in last 7 d” → vector + payload filter.`

**`Common output envelope`**

* `{"dry_run": true, "engine": "clickhouse|neo4j|qdrant", "query":"...", "expect":"rows|graph|ids"}`


  ### **`4) Automation Recommender`**

* `Input: HA entities (443), recent events, installed integrations. Output: ranked automation proposals with YAML or n8n JSON. Emphasize areas with 0 scripts and 6/7 cameras not yet using the Vision API.`

**`Output example`**

* `{`  
  *   `"proposals": [`  
  *     `{`  
  *       `"name": "night_motion_lights",`  
  *       `"impact": "medium",`  
  *       `"ha_yaml": "automation:\n  - alias: Night Motion Lights\n    trigger: ...",`  
  *       `"rollback": "disable automation id: night_motion_lights"`  
  *     `}`  
  *   `]`  
  * `}`  
    

    ### **`5) Storage Balancer`**

* `Policy: If any mount >85%, propose MinIO offload; at >95%, block new large pulls and require user confirm to free space. Current Models drive is at 94% → generate offload plan now.`

**`Action template`**

* `{`  
  *   `"action":"minio.move",`  
  *   `"source":"/models/flux",`  
  *   `"target":"s3://mini-sparx/models/flux",`  
  *   `"post_action":"ln -s /mnt/minio/models/flux /models/flux",`  
  *   `"dry_run": true`  
  * `}`  
      
  * 

